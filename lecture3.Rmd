---
title: "Introduction to Statistics"
subtitle: "Part III: Introduction to Regression and Pitfalls of Statistical Analysis"
author: "St√©phane Guerrier & Yuming Zhang"
date: "22 January 2021"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{R, setup, include = F}
# devtools::install_github("dill/emoGG")
library(pacman)
p_load(
  broom, tidyverse,
  latex2exp, ggplot2, ggthemes, ggforce, viridis, extrafont, gridExtra,
  kableExtra, snakecase, janitor,
  data.table, dplyr, estimatr,
  lubridate, knitr, parallel,
  lfe,
  here, magrittr
)
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
slate <- "#314f4f"
# Dark slate grey: #314f4f
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(crayon.enabled = F)
options(knitr.table.format = "html")
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18, family = "STIXGeneral"),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  text = element_text(family = "Fira Sans Book"),
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_set(theme_gray(base_size = 20))
# Column names for regression results
reg_columns <- c("Term", "Est.", "S.E.", "t stat.", "p-Value")
# Function for formatting p values
format_pvi <- function(pv) {
  return(ifelse(
    pv < 0.0001,
    "<0.0001",
    round(pv, 4) %>% format(scientific = F)
  ))
}
format_pv <- function(pvs) lapply(X = pvs, FUN = format_pvi) %>% unlist()
# Tidy regression results table
tidy_table <- function(x, terms, highlight_row = 1, highlight_color = "black", highlight_bold = T, digits = c(NA, 3, 3, 2, 5), title = NULL) {
  x %>%
    tidy() %>%
    select(1:5) %>%
    mutate(
      term = terms,
      p.value = p.value %>% format_pv()
    ) %>%
    kable(
      col.names = reg_columns,
      escape = F,
      digits = digits,
      caption = title
    ) %>%
    kable_styling(font_size = 20) %>%
    row_spec(1:nrow(tidy(x)), background = "white") %>%
    row_spec(highlight_row, bold = highlight_bold, color = highlight_color)
}
```

```{css, echo = F, eval = F}
@media print {
  .has-continuation {
    display: block !important;
  }
}
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::use_clipboard()
```

# Example: Reading ability assessment

.panelset[
.panel[.panel-name[Problem]
.smallest[An educator believes that new directed reading activities in the classroom can help elementary school students (6-12 years old) improve their reading ability. She arranged a pilot study where some students (chosen at random) of age 6 start to take part in these activities .hi-pink[(treatment group)], meanwhile other students continue with the classical curriculum .hi-purple[(control group)]. After three years, the educator wishes to evaluate the effectiveness of these activities so all students are given a Degree of Reading Power (DRP) test, which assesses their reading ability with scores given below: ]
```{r}
treatment_score = c(43, 48, 41, 53, 56, 58, 49, 54, 55, 52,
                    49, 42, 50, 60, 62, 64, 65, 58, 67, 69)
control_score = c(35, 43, 45, 50, 50, 46, 56, 63, 55, 63, 
                  58, 74, 60, 70, 72, 75, 69, 79, 82, 75)
```
.smallest[.pink[Can we conclude that these new directed reading activities can help elementary school students improve their reading ability?]]
]
.panel[.panel-name[Boxplot]
```{r, echo = F}
par(mai = c(1.5, 1.5, 0.5, 1))
boxplot(treatment_score, control_score, 
        names = c("Treatment", "Control"), 
        ylab = "Score of the DRP test",
        col = c("#F8766D", "#00BFC4"),
        cex.axis = 2, cex.lab = 2)
points(1:2, c(mean(treatment_score), mean(control_score)), 
       pch = 16, col = 1, cex = 2)
```
]
.panel[.panel-name[Boxplot Code]
```{r, eval = F}
boxplot(treatment_score, control_score, 
        names = c("Treatment", "Control"), 
        ylab = "Score of the DRP test",
        col = c("#F8766D", "#00BFC4"))
points(1:2, c(mean(treatment_score), mean(control_score)), 
       pch = 16, col = 1)
```
]
.panel[.panel-name[Test]
1. .purple[Define hypotheses:] $H_0: \mu_t = \mu_c$ and $H_a: \mu_t > \mu_c$.
2. .purple[Define] $\color{#6A5ACD}{\alpha}$: We consider $\alpha = 5\%$.
3. .purple[Compute p-value]: p-value = $95.78\%$ (see R code tab for details).
4. .purple[Conclusion:] We have p-value > $\alpha$ so we cannot reject the null hypothesis at the significance level of 5%. 

.hi-pink[Remark:] Notice that the p-value for the test with opposite hypotheses is actually $1-95.78\%=4.22\% < \alpha$. So we conclude, at the significance level of 5%, that the classical curriculum without these new directed reading activities actually improve students' reading ability more compared to the curriculum with these activities. ü§î
]
.panel[.panel-name[`R` Code ]
```{r}
t.test(treatment_score, control_score, alternative = "greater")
```
]
]

---

# Is our analysis comprehensive?

The educator points out that she has considered these new directed reading activities for only three years. So only students of 6-8 years old have participated in these activities. In other words, in the sample she collected, the students in the treatment group are only of age 6 to 8, whereas the students in the control group vary from 6 to 12 years old. .pink[Is age a potential explanation to the difference we observe among the students' reading ability?]

To make sure that the analysis is reliable, she includes the age information of the students as follows:
```{r}
treatment_age = c(6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
                  7, 7, 7, 7, 7, 7, 7, 8, 8, 8)
control_age = c(6, 6, 7, 7, 8, 8, 8, 8, 8, 9, 9, 
                10, 10, 10, 10, 11, 11, 12, 12, 12)
```

---

# Should age be taken into account?

```{r, echo = F}
# plot
par(mai = c(2, 2, 1, 1))
plot(treatment_age, treatment_score, pch = 16, col = "#F8766D", cex = 1.5,
     xlim = c(6,12), ylim = c(10, 100), 
     xlab = "Age (in years)", ylab = "Score of the DRP test",
     cex.axis = 2, cex.lab = 2)
points(control_age, control_score, pch = 17, col = "#00BFC4", cex = 1.5)

# lines(6:12, treatment_mod$coefficients[1] + treatment_mod$coefficients[2]*(6:12),
#       lwd = 1.5, col = "#F8766D")
# lines(6:12, control_mod$coefficients[1] + control_mod$coefficients[2]*(6:12), 
#       lwd = 1.5, col = "#00BFC4")

legend("bottomright",
       legend = c("Treatment", "Control"),
       col = c("#F8766D", "#00BFC4"),
       bty = "n",
       # lty = c(1, 1),
       # lwd = c(1.5, 1.5),
       pch = c(16, 17),
       pt.cex = c(1.5, 1.5),
       cex = 2)
```

---

# Regression models

- .smallest[.hi-pink[Regression models] study the effects of some explanatory variables (also called covariates or regressors)] $\small X_1, \ldots, X_p$ .smallest[on a response variable] $\small Y$ .smallest[of primary interest.]  
- .smallest[The relationship between the response variable] $\small Y$ .smallest[and the covariates is not deterministic. Instead, it shows some random errors. In other words, the response] $\small Y$ .smallest[is random and .purple[its mean value is modelled as a function of the covariates.]] 
- .smallest[In .hi-pink[linear regression models], the mean value of] $\small Y$ .smallest[is a linear combination of the covariates:] 
$$Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2).$$
- .smallest[.purple[The random errors are assumed to be independently and identically normally distributed with mean 0 and variance]] $\small \color{#6A5ACD}{\sigma^2}$ .smallest[.purple[.]]
- .smallest[The parameters of interest are] $\small \beta_0, \beta_1, \ldots, \beta_p$<sup>.smallest[üëã]</sup>.smallest[. They are estimated using the least squares approach.]

.footnote[.smallest[üëã Here we consider a simple situation where the variance is known. In practice, however, the variance is typically unknown so it needs to be estimated as well.]]

---

# Example: Reading ability assessment

In the reading ability example, we can formulate a linear regression model as follows:
$$\color{#e64173}{\text{Score}_i} = \beta_0 + \beta_1 \color{#6A5ACD}{\text{Group}_i} + \beta_2 \color{#20B2AA}{\text{Age}_i} + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2).$$
- $\color{#e64173}{\text{Score}_i}$: score of the DRP test of the $i$-th student.
- $\color{#6A5ACD}{\text{Group}_i}$: indicator of participation of the new directed reading activities for the $i$-th student (i.e. $\color{#6A5ACD}{\text{Group}_i} = 1$ if participate and $\color{#6A5ACD}{\text{Group}_i} = 0$ if not participate).
- $\color{#20B2AA}{\text{Age}_i}$: age of the $i$-th student. 

With this model the two groups can be compared as the age effect is taken into account. The goal of the educator is now to assess if $\beta_1$ is .pink[significantly larger than 0] (why??? ü§î).

---

# Example: Reading ability assessment

.panelset[
.panel[.panel-name[R Code]
```{r, eval = F}
# combine all available data together
reading_data = data.frame(score = c(treatment_score, control_score),
                          group = c(rep(1,20), rep(0, 20)),
                          age = c(treatment_age, control_age))

# fit a linear regression model
mod = lm(score ~ group + age, data = reading_data)
summary(mod)
```
]
.panel[.panel-name[Output]
```{r, echo = F}
# combine all available data together
reading_data = data.frame(score = c(treatment_score, control_score),
                          group = c(rep(1,20), rep(0, 20)),
                          age = c(treatment_age, control_age))

# fit a linear regression model
mod = lm(score ~ group + age, data = reading_data)
summary(mod)
```
]
.panel[.panel-name[Graph Code]
```{r, eval = F}
par(mai = c(2, 2, 1, 1))
plot(treatment_age, treatment_score, pch = 16, col = "#F8766D",
     xlim = c(6,12), ylim = c(10, 100), 
     xlab = "Age (in years)", ylab = "Score of the DRP test")
points(control_age, control_score, pch = 17, col = "#00BFC4")

lines(6:12, coef(mod)[1]+coef(mod)[2]*1+coef(mod)[3]*(6:12), col = "#F8766D")
lines(6:12, coef(mod)[1]+coef(mod)[2]*0+coef(mod)[3]*(6:12), col = "#00BFC4")

legend("bottomright",
       legend = c("Treatment", "Control"),
       col = c("#F8766D", "#00BFC4"),
       bty = "n",
       pch = c(16, 17))
```
]
.panel[.panel-name[Graph]
```{r, echo = F}
# plot
par(mai = c(2, 2, 1, 1))
plot(treatment_age, treatment_score, pch = 16, col = "#F8766D", cex = 1.5,
     xlim = c(6,12), ylim = c(10, 100), 
     xlab = "Age (in years)", ylab = "Score of the DRP test",
     cex.axis = 2, cex.lab = 2)
points(control_age, control_score, pch = 17, col = "#00BFC4", cex = 1.5)

lines(6:12, coef(mod)[1]+coef(mod)[2]*1+coef(mod)[3]*(6:12), 
      lwd = 1.5, col = "#F8766D")
lines(6:12, coef(mod)[1]+coef(mod)[2]*0+coef(mod)[3]*(6:12), 
      lwd = 1.5, col = "#00BFC4")

legend("bottomright",
       legend = c("Treatment", "Control"),
       col = c("#F8766D", "#00BFC4"),
       bty = "n",
       lty = c(1, 1),
       lwd = c(1.5, 1.5),
       pch = c(16, 17),
       pt.cex = c(1.5, 1.5),
       cex = 2)
```
]
]

---

# Interpretation of coefficients

We can obtain the estimated coefficients. Specifically,
- $\hat{\beta}_0 = 1.6263$ represents the estimated baseline average score of the DRP test.
- $\hat{\beta}_1 = 9.7352$ means that .pink[for a student of the same age], participating the new directed reading activities is estimated to increase their average score of the DRP test by 9.7352.
- $\hat{\beta}_2 = 6.5246$ means that .pink[when a student receives the same treatment] (either participate or not in the activities), their average score increases by 6.5246 as they become 1 year older. 

Regression coefficients represent the mean change in the response variable .purple[for one unit of change] in the predictor variable .purple[while holding other covariates in the model constant.]

---

# Interpretation of coefficient p-values

- We notice that for each coefficient $\beta_i$, there is a corresponding p-value.
- This coefficient p-value is associated to the test of $H_0: \beta_i = 0$ and $H_a: \beta_i \neq 0$. 
- .pink[A covariate with a small p-value (typically smaller than 5%) is considered to be a significant (meaningful) addition to the model], as changes in the values of such covariate can lead to changes in the response variable. 
- On the other hand, a large p-value (typically larger than 5%) suggests that the corresponding covariate is not significantly associated with changes in the response. 
- In this example, the coefficient p-value associated to the "group" covariate is 0.0293% (smaller than 5%). .purple[This suggests that taking into account the effect of age, the reading abilities of the students receiving the treatment is significantly .hi.purple[different] from the control group, at the significance level of 5%.] But this is not what we want! üò§ 

---

# Interpretation of coefficient p-values

.smaller[In the linear regression output, the coefficient p-value (which we denote as] $\small p$.smaller[) corresponds to a two-sided test. We can use this result to compute the p-value of a one-sided test using the following relations:]

|               | $\small H_a: \beta_i>0$   | $\small H_a: \beta_i<0$  |
| ------------- |:-------------:| :-----:|
| $\small \hat{\beta_i}>0$     | $\small p/2$ | $\small 1-p/2$ |
| $\small \hat{\beta_i}<0$      | $\small 1-p/2$      |   $\small p/2$ |

.smaller[In our example,] $\small \beta_1 = 9.7352$ .smaller[and] $\small p=0.0293\%$.smaller[. So the p-value of the test with hypotheses] $\small H_0: \beta_1=0$ .smaller[and] $\small H_a: \beta_1>0$ .smaller[is] $\small 0.0293\% /2 \approx 0.015\% < \alpha$.smaller[. .pink[So we can conclude that these new directed reading activities can significantly improve students' reading ability compared to classical curriculum.] üòç]

---

# Interpretation of R-squared

- The .hi-pink[coefficient of determination], denoted as $R^2$ and often referred to as R-squared, is the proportion of the variance in the response variable that is predictable from the covariates.  
- $\color{#6A5ACD}{R^2}$ .purple[gives certain information about the goodness of fit of a model.] It measures how well the regression predictions approximate the real data points. An $R^2$ of 1 indicates that the regression predictions perfectly fit the data.
- However, adding new covariates to the current model .purple[always] increases $R^2$, whether the additional covariates are significant or not. Therefore, $R^2$ alone cannot be used as a meaningful comparison of models with different covariates.
- The .hi-pink[adjusted] $\color{#e64173}{R^2}$ is a modification of $R^2$ that aims to limit this issue. 

---

# Exercise: Is gender also significant?

.panelset[
.panel[.panel-name[Problem]
.pink[The educator thinks that the reading ability may also vary between male and female students.] She further provided the gender information of students, which can be imported as follows:
```{r}
# gender information: 1 = female, 0 = male
treatment_gender = c(1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
                     1, 0, 0, 1, 1, 1, 1, 0, 0, 1)
control_gender = c(0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
                   1, 1, 0, 0, 0, 1, 1, 0, 1, 0)

# combine all available data together
reading_data = data.frame(score = c(treatment_score, control_score),
                          group = c(rep(1,20), rep(0, 20)),
                          age = c(treatment_age, control_age),
                          gender = c(treatment_gender, control_gender))
```
]
.panel[.panel-name[R Code]
```{r, eval = F}
# fit a linear regression model with added "gender" covariate
mod = lm(score ~ group + age + gender, data = reading_data)
summary(mod)
```
]
.panel[.panel-name[Output]
```{r, echo = F}
# fit a linear regression model with added "gender" covariate
mod = lm(score ~ group + age + gender, data = reading_data)
summary(mod)
```
]
]

---

# How to make predictions?

.smallest[Given the promising improvement on students' reading ability with these new directed reading activities, the educator plans to continue involving her students to participate in these activities. She is interested in predicting .pink[how a student, who follows curriculum with these activities, will perform in the DRP test at the end of their elementary school studies (i.e. at 12 years old)?]]

.smallest[In general, to predict the value of the response variable for a future observation with the value] $\small x_i^*$ .smallest[of the explanatory variables, we use]
$$\small \hat{y}^* = \hat{\beta}_0 + \hat{\beta}_1 x_1^* + \ldots + \hat{\beta}_p x_p^*.$$

.smallest[In this case, recall that with the model] $\small \color{#e64173}{\text{Score}} = \beta_0 + \beta_1 \color{#6A5ACD}{\text{Group}} + \beta_2 \color{#20B2AA}{\text{Age}} + \epsilon$, .smallest[we have] $\small \hat{\beta}_0 = 1.6263$, $\small \hat{\beta}_1 = 9.7352$ .smallest[and] $\small \hat{\beta}_2 = 6.5246$.smallest[. So the predicted score of the DRP test for a 12 year-old student with participation in these activities can be computed as]
$$\small 1.6263 + 9.7352 \cdot \color{#6A5ACD}{1} + 6.5246 \cdot \color{#20B2AA}{12} \approx \color{#e64173}{89.66}.$$

---

# Uncertainty of predictions

- Similar to point estimation, prediction is also .hi-turquoise[random] as it relies on estimation $\hat{\beta}_i$ that is computed on a random sample.
- Moreover, there exists a variability of the future response around the mean due to the unavoidable random errors of the model. 
- To measure the uncertainty of the prediction, we use the .hi-pink[prediction intervals].
- Prediction intervals correspond to .purple[a range of values that are likely to include the true value of a future observation.] It includes a margin of error to indicate its accuracy. The level of accuracy is a probability expressed as a percentage (%).
- A 95% prediction interval means that the probability of using this model to produce an interval that contains the true value of the future observation is 95%. 

---

# Prediction intervals

.panelset[
.panel[.panel-name[Problem]
The educator is interested in comparing how a student will perform in the DRP test at the end of the elementary school studies (i.e. at 12 years old), with and without these new directed reading activities, with probability of 95%. 

We predict, with the probability of 95%, that for 12 year-old students who particiate in these activities, their scores of the DRP test may fall in the range of .pink[(75.66, 103.65)]. On the other hand, for those who do not participate in these activities, their scores may fall in the range of .purple[(67.24, 92.60)]. (See R code tab for details.)
]
.panel[.panel-name[R Code]
```{r}
# with treatment
mod = lm(score ~ group + age, data = reading_data)
new_data = data.frame(group = 1, age = 12)
predict(mod, new_data, interval = "predict")
```
```{r}
# without treatment
mod = lm(score ~ group + age, data = reading_data)
new_data = data.frame(group = 0, age = 12)
predict(mod, new_data, interval = "predict")
```
]
]

---

# Limitations of linear regression models

- .smaller[Linear regression .pink[assumes a linear relationship] between the response and the covariates. However this is sometimes incorrect.]
- .smaller[Linear regression .pink[is applicable when the response variable is continuous and approximately normally distributed.] So it is not suitable when, for example, we need to analyze binary data or count data. In this case, a more general class of model called the .purple[generalized linear model] can be used.]
- .smaller[Linear regression .pink[only considers independent data.] So it is not suitable, for example, for time dependent data or clustered data. For these cases, we may use .purple[time series analysis] or .purple[mixed effect model] respectively.]
- .smaller[Linear regression .pink[should not be used to extrapolate], i.e. to estimate beyond the original observation range. For example, if we consider a 100 year-old person in this reading ability example, we would predict that the corresponding score of the DRP test would be 663.82 and 654.08, respectively, with and without these activities. Does it really make sense? üòØ]

---

# Limitations of linear regression models

```{R, out.width = "80%", echo = F}
include_graphics("pics/extrapolating.png")
```

üëã .smallest[If you want to know more have a look [here](https://www.explainxkcd.com/wiki/index.php/605:_Extrapolating).]

---

# Pitfalls of statistical analysis

.smallest[.hi-pink[P-Hacking] refers to the misuse of data analysis to find patterns in data that can be presented as statistically significant. .turquoise[This is done by performing many statistical tests on the data and only reporting those that come back with significant results]<sup>.smallest[üëã]</sup>. .hi-purple[Example]: a correlation between the number of letters in [Scripps National Spelling Bee's](https://en.wikipedia.org/wiki/Scripps_National_Spelling_Bee) winning word and the number of people in the United States killed by venomous spiders.]

```{R, phacking, out.width = "82%", echo = F}
include_graphics("pics/phacking.png")
```

.smallest[Source: [Wikipedia on Data dredging ](https://en.wikipedia.org/wiki/Data_dredging)]. üëã .smallest[If you want to know more, have a look [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1124898/).]

---

# P-Hacking 

<div align="center">
<iframe width="784" height="441" src="http://nsfwyoutube.com/watchmore?v=0Rnq1NpHdmw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

.tiny[Source: Scientific Studies: Last Week Tonight with John Oliver (HBO)]

üëã .smallest[If you want to know more, have a look [here](https://fivethirtyeight.com/features/you-cant-trust-what-you-read-about-nutrition/).]

---

# How to P-Hack? ü§î

.smaller[Are republicans are good or bad for the American Ecy? .hi-purple[Both!!] (see [link](https://projects.fivethirtyeight.com/p-hacking/))]

```{R, rep, out.width = "72%", echo = F}
include_graphics("pics/republican.png")
```

---

# This bring us to "HARKing"

.pull-left[.smallsest[.hi-pink[HARKing] is an acronym coined by Norbert Kerr for the .purple[questionable research practice of "Hypothesizing After the Results are Known"]. It can be defined as presenting .hi-turquoise[a post hoc] hypothesis in the introduction of a research report as if it were an .hi-turquoise[a priori] hypothesis.

.hi-pink[How to do it?] ü§î
- Test if one jelly bean color is linked to acne
- Pretend this was our original hypothesis
- Et voil√†!

üëã  If you want to know more, have a look [here](https://en.wikipedia.org/wiki/HARKing) and the references therein.]]


.pull-right[
<br>
```{R, greenagainagain, out.width = "90%", echo = F}
include_graphics("pics/green.png")
```
.tiny[Source: [xkcd](https://xkcd.com/882/)]
]

---

# The costs of HACKing (and P-Hacking)

HARKing is (arguably) playing an important role in the .hi-purple[replication crisis in science]!  

<br>
```{R, replic, out.width = "80%", echo = F}
include_graphics("pics/replic.jpg")
```

.tiny[Source: [Open Science Collaboration. "Estimating the reproducibility of psychological science." Science 349.6251 (2015)](https://science.sciencemag.org/content/sci/349/6251/aac4716.full.pdf?casa_token=2i_9tsA7a-QAAAAA:B-N4J8bk1hBRboZAmPKuythdhFcdhKKNOtBATZX1PD69qhV-cl1zZG3zcd25LkhnKMpRrmO9TJAxo3f8).]

üëã  .smallest[If you want to know more, have a look [here](https://en.wikipedia.org/wiki/Replication_crisis) and the references therein.]

---

# ‚ö†Ô∏è Take home message

- Any data analysis made on a sample of data is subject to .purple[randomness].
- Understanding the random processes underlying any data analysis, includinunderlyingg statistics such as confidence intervals and p-values, requires basic knowledge in probability and statistics, or .pink[statistical literacy].
- With data, there is never 100% certainty, but rather a .pink[conclusion associated to a (estimated) risk] that the conclusion might be wrong. 
- .pink[One has to live with this fact, and any scientific activity that might conclude with 100% certainty on the basis of data, is simply a fallacy].
- However, properly controlling for the statistical risk and properly rstating the conclusions that can be drawn from a data analysis, can really bring new knowledge, especially in all the sciences that are based on observations.
